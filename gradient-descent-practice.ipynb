{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-22T11:18:03.448670Z","iopub.execute_input":"2023-06-22T11:18:03.449193Z","iopub.status.idle":"2023-06-22T11:18:03.456587Z","shell.execute_reply.started":"2023-06-22T11:18:03.449157Z","shell.execute_reply":"2023-06-22T11:18:03.455568Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### 보스턴 주택 가격 데이터 세트를 Peceptron 기반에서 학습 및 테스트하기 위한 데이터 로드\n* 사이킷런에서 보스턴 주택 가격 데이터 세트를 로드하고 이를 DataFrame으로 생성","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_boston\n\nboston = load_boston()\ndata_url = \"http://lib.stat.cmu.edu/datasets/boston\"\nboston = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\nbostonDF = pd.DataFrame(boston.data, columns=boston.feature_names)\nbostonDF['PRICE'] = boston.target\nprint(bostonDF.shape)\nbostonDF.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-22T11:18:07.015818Z","iopub.execute_input":"2023-06-22T11:18:07.017126Z","iopub.status.idle":"2023-06-22T11:18:07.634557Z","shell.execute_reply.started":"2023-06-22T11:18:07.017072Z","shell.execute_reply":"2023-06-22T11:18:07.632818Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_boston\n\u001b[1;32m      3\u001b[0m boston \u001b[38;5;241m=\u001b[39m load_boston()\n\u001b[1;32m      4\u001b[0m data_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://lib.stat.cmu.edu/datasets/boston\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/datasets/__init__.py:156\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_boston\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    106\u001b[0m     msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n","\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"],"ename":"ImportError","evalue":"\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n","output_type":"error"}]},{"cell_type":"markdown","source":"### Weight와 Bias의 Update 값을 계산하는 함수 생성.\n* w1은 RM(방의 계수) 피처의 Weight 값\n* w2는 LSTAT(하위계층 비율) 피처의 Weight 값\n* bias는 Bias\n* N은 입력 데이터 건수\n![](https://raw.githubusercontent.com/chulminkw/CNN_PG/main/utils/images/Weight_update.png)\n","metadata":{}},{"cell_type":"code","source":"# gradient_descent()함수에서 반복적으로 호출되면서 update될 weight/bias 값을 계산하는 함수. \n# rm은 RM(방 개수), lstat(하위계층 비율), target은 PRICE임. 전체 array가 다 입력됨. \n# 반환 값은 weight와 bias가 update되어야 할 값과 Mean Squared Error 값을 loss로 반환.\n\ndef get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate):\n    N = len(target)\n    \n    predicted = w1 * rm + w2 * lstat + bias\n    diff = target - predicted\n    bias_factors = np.ones((N,))\n    \n    w1_update = -(2/N) * learning_rate*(np.dot(rm.T, diff))#numpy dot 연산\n    w2_update = -(2/N) * learning_rate * (np.dot(lstat.T,diff))\n    bias_update = -(2/N) * learning_rate*(np.dot(bias_factors.T,diff))\n    \n    mse_loss = np.mean(np.square(diff))\n    \n    return bias_update, w1_update, w2_update, mse_loss","metadata":{"execution":{"iopub.status.busy":"2023-06-22T07:52:58.577472Z","iopub.execute_input":"2023-06-22T07:52:58.577881Z","iopub.status.idle":"2023-06-22T07:52:58.585926Z","shell.execute_reply.started":"2023-06-22T07:52:58.577851Z","shell.execute_reply":"2023-06-22T07:52:58.584722Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Descent 를 적용하는 함수 생성\n* iter_epochs 수만큼 반복적으로 get_update_weights_value()를 호출하여 update될 weight/bias값을 구한 뒤 Weight/Bias를 Update적용. ","metadata":{}},{"cell_type":"code","source":"# RM, LSTAT feature array와 PRICE target array를 입력 받아서 iter_epochs수만큼 반복적으로 Weight와 Bias를 update적용. \ndef gradient_descent(features, target, iter_epochs=1000, verbose=True):\n    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n    w1 = np.zeros((1,))#원래는 랜덤값으로 해야됨.\n    w2 = np.zeros((1,))\n    bias = np.zeros((1, ))\n    print('최초 w1, w2, bias:', w1, w2, bias)\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    learning_rate = 0.01\n    rm = features[:, 0]\n    lstat = features[:, 1]\n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n    for i in range(iter_epochs):\n        # weight/bias update 값 계산 \n        bias_update, w1_update, w2_update, loss = get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate)\n        # weight/bias의 update 적용. \n        w1 = w1 - w1_update\n        w2 = w2 - w2_update\n        bias = bias - bias_update\n        if verbose:\n            print('Epoch:', i+1,'/', iter_epochs)\n            print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', loss)\n        \n    return w1, w2, bias","metadata":{"execution":{"iopub.status.busy":"2023-06-22T07:55:11.013458Z","iopub.execute_input":"2023-06-22T07:55:11.013887Z","iopub.status.idle":"2023-06-22T07:55:11.025252Z","shell.execute_reply.started":"2023-06-22T07:55:11.013854Z","shell.execute_reply":"2023-06-22T07:55:11.024112Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Descent 적용\n* 신경망은 데이터를 정규화/표준화 작업을 미리 선행해 주어야 함. \n* 이를 위해 사이킷런의 MinMaxScaler를 이용하여 개별 feature값은 0~1사이 값으로 변환후 학습 적용.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaled_features = scaler.fit_transform(bostonDF[['RM', 'LSTAT']])\n\nw1, w2, bias = gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=5000, verbose=True)\nprint('##### 최종 w1, w2, bias #######')\nprint(w1, w2, bias)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 계산된 Weight와 Bias를 이용하여 Price 예측\n* 예측 feature 역시 0~1사이의 scaled값을 이용하고 Weight와 bias를 적용하여 예측값 계산. ","metadata":{}},{"cell_type":"code","source":"predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\nbostonDF['PREDICTED_PRICE'] = predicted\nbostonDF.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Keras를 이용하여 보스턴 주택가격 모델 학습 및 예측\n* Dense Layer를 이용하여 퍼셉트론 구현. units는 1로 설정. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\n\nmodel = Sequential([\n    # 단 하나의 units 설정. input_shape는 2차원, 회귀이므로 activation은 설정하지 않음. \n    # weight와 bias 초기화는 kernel_inbitializer와 bias_initializer를 이용. \n    Dense(1, input_shape=(2, ), activation=None, kernel_initializer='zeros', bias_initializer='ones')\n])\n#input_shape이 (2,)는 칼럼이 2개인 데이터. 즉, RM, lstat이다. 회귀는 활성함수가 필요없다.\n#kernel_initializer는 weight를 초기화하는 거고, bias_initializer는 bias를 초기화한다.\n\n# Adam optimizer를 이용하고 Loss 함수는 Mean Squared Error, 성능 측정 역시 MSE를 이용하여 학습 수행. \nmodel.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mse'])\nmodel.fit(scaled_features, bostonDF['PRICE'].values, epochs=1000)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T11:14:09.337459Z","iopub.execute_input":"2023-06-22T11:14:09.337821Z","iopub.status.idle":"2023-06-22T11:14:20.313344Z","shell.execute_reply.started":"2023-06-22T11:14:09.337788Z","shell.execute_reply":"2023-06-22T11:14:20.311583Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Adam optimizer를 이용하고 Loss 함수는 Mean Squared Error, 성능 측정 역시 MSE를 이용하여 학습 수행. \u001b[39;00m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 12\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mscaled_features\u001b[49m, bostonDF[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPRICE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'scaled_features' is not defined"],"ename":"NameError","evalue":"name 'scaled_features' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"### Keras로 학습된 모델을 이용하여 주택 가격 예측 수행. ","metadata":{}},{"cell_type":"code","source":"predicted = model.predict(scaled_features)\nbostonDF['KERAS_PREDICTED_PRICE'] = predicted\nbostonDF.head(10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stochastic Gradient Descent와 Mini Batch Gradient Descent 구현\n* SGD 는 전체 데이터에서 한건만 임의로 선택하여 Gradient Descent 로 Weight/Bias Update 계산한 뒤 Weight/Bias 적용\n* Mini Batch GD는 전체 데이터에서 Batch 건수만큼 데이터를 선택하여 Gradient Descent로 Weight/Bias Update 계산한 뒤 Weight/Bias 적용","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\nbostonDF = pd.DataFrame(boston.data, columns=boston.feature_names)\nbostonDF['PRICE'] = boston.target\nprint(bostonDF.shape)\nbostonDF.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-22T11:40:11.847324Z","iopub.execute_input":"2023-06-22T11:40:11.847808Z","iopub.status.idle":"2023-06-22T11:40:11.915223Z","shell.execute_reply.started":"2023-06-22T11:40:11.847778Z","shell.execute_reply":"2023-06-22T11:40:11.913700Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_boston\n\u001b[1;32m      5\u001b[0m boston \u001b[38;5;241m=\u001b[39m load_boston()\n\u001b[1;32m      6\u001b[0m bostonDF \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(boston\u001b[38;5;241m.\u001b[39mdata, columns\u001b[38;5;241m=\u001b[39mboston\u001b[38;5;241m.\u001b[39mfeature_names)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/datasets/__init__.py:156\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_boston\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    106\u001b[0m     msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n","\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"],"ename":"ImportError","evalue":"\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n","output_type":"error"}]},{"cell_type":"markdown","source":"### SGD 기반으로 Weight/Bias update 값 구하기","metadata":{}},{"cell_type":"code","source":"def get_update_weights_value_sgd(bias, w1, w2, rm_sgd, lstat_sgd, target_sgd, learning_rate=0.01):\n    \n    # 데이터 건수\n    N = target_sgd.shape[0]\n    # 예측 값. \n    predicted_sgd = w1 * rm_sgd + w2*lstat_sgd + bias\n    # 실제값과 예측값의 차이 \n    diff_sgd = target_sgd - predicted_sgd\n    # bias 를 array 기반으로 구하기 위해서 설정. \n    bias_factors = np.ones((N,))\n    \n    # weight와 bias를 얼마나 update할 것인지를 계산.  \n    w1_update = -(2/N)*learning_rate*(np.dot(rm_sgd.T, diff_sgd))\n    w2_update = -(2/N)*learning_rate*(np.dot(lstat_sgd.T, diff_sgd))\n    bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff_sgd))\n    \n    # Mean Squared Error값을 계산. \n    #mse_loss = np.mean(np.square(diff))\n    \n    # weight와 bias가 update되어야 할 값 반환 \n    return bias_update, w1_update, w2_update","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SGD 수행하기","metadata":{}},{"cell_type":"code","source":"print(bostonDF['PRICE'].values.shape)\nprint(np.random.choice(bostonDF['PRICE'].values.shape[0], 1))\nprint(np.random.choice(506, 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RM, LSTAT feature array와 PRICE target array를 입력 받아서 iter_epochs수만큼 반복적으로 Weight와 Bias를 update적용. \ndef st_gradient_descent(features, target, iter_epochs=1000, verbose=True):\n    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n    np.random.seed = 2021\n    w1 = np.zeros((1,))\n    w2 = np.zeros((1,))\n    bias = np.zeros((1, ))\n    print('최초 w1, w2, bias:', w1, w2, bias)\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    learning_rate = 0.01\n    rm = features[:, 0]\n    lstat = features[:, 1]\n    \n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n    for i in range(iter_epochs):\n        # iteration 시마다 stochastic gradient descent 를 수행할 데이터를 한개만 추출. 추출할 데이터의 인덱스를 random.choice() 로 선택. \n        stochastic_index = np.random.choice(target.shape[0], 1)\n        rm_sgd = rm[stochastic_index]\n        lstat_sgd = lstat[stochastic_index]\n        target_sgd = target[stochastic_index]\n        # SGD 기반으로 Weight/Bias의 Update를 구함.  \n        bias_update, w1_update, w2_update = get_update_weights_value_sgd(bias, w1, w2, rm_sgd, lstat_sgd, target_sgd, learning_rate)\n        \n        # SGD로 구한 weight/bias의 update 적용. \n        w1 = w1 - w1_update\n        w2 = w2 - w2_update\n        bias = bias - bias_update\n        if verbose:\n            print('Epoch:', i+1,'/', iter_epochs)\n            # Loss는 전체 학습 데이터 기반으로 구해야 함.\n            predicted = w1 * rm + w2*lstat + bias\n            diff = target - predicted\n            mse_loss = np.mean(np.square(diff))\n            print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', mse_loss)\n        \n    return w1, w2, bias","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaled_features = scaler.fit_transform(bostonDF[['RM', 'LSTAT']])\n\nw1, w2, bias = st_gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=5000, verbose=True)\nprint('##### 최종 w1, w2, bias #######')\nprint(w1, w2, bias)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\nbostonDF['PREDICTED_PRICE_SGD'] = predicted\nbostonDF.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### iteration시마다 일정한 batch 크기만큼의 데이터를 random하게 가져와서 GD를 수행하는 Mini-Batch GD 수행","metadata":{}},{"cell_type":"code","source":"def get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate=0.01):\n    \n    # 데이터 건수\n    N = target_batch.shape[0]\n    # 예측 값. \n    predicted_batch = w1 * rm_batch+ w2 * lstat_batch + bias\n    # 실제값과 예측값의 차이 \n    diff_batch = target_batch - predicted_batch\n    # bias 를 array 기반으로 구하기 위해서 설정. \n    bias_factors = np.ones((N,))\n    \n    # weight와 bias를 얼마나 update할 것인지를 계산.  \n    w1_update = -(2/N)*learning_rate*(np.dot(rm_batch.T, diff_batch))\n    w2_update = -(2/N)*learning_rate*(np.dot(lstat_batch.T, diff_batch))\n    bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff_batch))\n    \n    # Mean Squared Error값을 계산. \n    #mse_loss = np.mean(np.square(diff))\n    \n    # weight와 bias가 update되어야 할 값 반환 \n    return bias_update, w1_update, w2_update","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_indexes = np.random.choice(506, 30)\nprint(batch_indexes)\n\nbostonDF['RM'].values[batch_indexes]\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# batch_gradient_descent()는 인자로 batch_size(배치 크기)를 입력 받음. \ndef batch_random_gradient_descent(features, target, iter_epochs=1000, batch_size=30, verbose=True):\n    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n    np.random.seed = 2021\n    w1 = np.zeros((1,))\n    w2 = np.zeros((1,))\n    bias = np.zeros((1, ))\n    print('최초 w1, w2, bias:', w1, w2, bias)\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    learning_rate = 0.01\n    rm = features[:, 0]\n    lstat = features[:, 1]\n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n    for i in range(iter_epochs):\n        # batch_size 갯수만큼 데이터를 임의로 선택. \n        batch_indexes = np.random.choice(target.shape[0], batch_size)\n        rm_batch = rm[batch_indexes]\n        lstat_batch = lstat[batch_indexes]\n        target_batch = target[batch_indexes]\n        # Batch GD 기반으로 Weight/Bias의 Update를 구함. \n        bias_update, w1_update, w2_update = get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate)\n        \n        # Batch GD로 구한 weight/bias의 update 적용. \n        w1 = w1 - w1_update\n        w2 = w2 - w2_update\n        bias = bias - bias_update\n        if verbose:\n            print('Epoch:', i+1,'/', iter_epochs)\n            # Loss는 전체 학습 데이터 기반으로 구해야 함.\n            predicted = w1 * rm + w2*lstat + bias\n            diff = target - predicted\n            mse_loss = np.mean(np.square(diff))\n            print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', mse_loss)\n        \n    return w1, w2, bias","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1, w2, bias = batch_random_gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=5000, batch_size=30, verbose=True)\nprint('##### 최종 w1, w2, bias #######')\nprint(w1, w2, bias)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\nbostonDF['PREDICTED_PRICE_BATCH_RANDOM'] = predicted\nbostonDF.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### iteration 시에 순차적으로 일정한 batch 크기만큼의 데이터를 전체 학습데이터에 걸쳐서 가져오는 Mini-Batch GD 수행","metadata":{}},{"cell_type":"code","source":"for batch_step in range(0, 506, 30):\n    print(batch_step)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bostonDF['PRICE'].values[480:510]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# batch_gradient_descent()는 인자로 batch_size(배치 크기)를 입력 받음. \ndef batch_gradient_descent(features, target, iter_epochs=1000, batch_size=30, verbose=True):\n    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n    np.random.seed = 2021\n    w1 = np.zeros((1,))\n    w2 = np.zeros((1,))\n    bias = np.zeros((1, ))\n    print('최초 w1, w2, bias:', w1, w2, bias)\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    learning_rate = 0.01\n    rm = features[:, 0]\n    lstat = features[:, 1]\n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n    for i in range(iter_epochs):\n        # batch_size 만큼 데이터를 가져와서 weight/bias update를 수행하는 로직을 전체 데이터 건수만큼 반복\n        for batch_step in range(0, target.shape[0], batch_size):\n            # batch_size만큼 순차적인 데이터를 가져옴. \n            rm_batch = rm[batch_step:batch_step + batch_size]\n            lstat_batch = lstat[batch_step:batch_step + batch_size]\n            target_batch = target[batch_step:batch_step + batch_size]\n        \n            bias_update, w1_update, w2_update = get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate)\n\n            # Batch GD로 구한 weight/bias의 update 적용. \n            w1 = w1 - w1_update\n            w2 = w2 - w2_update\n            bias = bias - bias_update\n        \n            if verbose:\n                print('Epoch:', i+1,'/', iter_epochs, 'batch step:', batch_step)\n                # Loss는 전체 학습 데이터 기반으로 구해야 함.\n                predicted = w1 * rm + w2*lstat + bias\n                diff = target - predicted\n                mse_loss = np.mean(np.square(diff))\n                print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', mse_loss)\n        \n    return w1, w2, bias","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w1, w2, bias = batch_gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=5000, batch_size=30, verbose=True)\nprint('##### 최종 w1, w2, bias #######')\nprint(w1, w2, bias)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\nbostonDF['PREDICTED_PRICE_BATCH'] = predicted\nbostonDF.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mini BATCH GD를 Keras로 수행\n* Keras는 기본적으로 Mini Batch GD를 수행","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\n\nmodel = Sequential([\n    # 단 하나의 units 설정. input_shape는 2차원, 회귀이므로 activation은 설정하지 않음. \n    # weight와 bias 초기화는 kernel_inbitializer와 bias_initializer를 이용. \n    Dense(1, input_shape=(2, ), activation=None, kernel_initializer='zeros', bias_initializer='ones')\n])\n# Adam optimizer를 이용하고 Loss 함수는 Mean Squared Error, 성능 측정 역시 MSE를 이용하여 학습 수행. \nmodel.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mse'])\n\n# Keras는 반드시 Batch GD를 적용함. batch_size가 None이면 32를 할당. \nmodel.fit(scaled_features, bostonDF['PRICE'].values, batch_size=30, epochs=1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = model.predict(scaled_features)\nbostonDF['KERAS_PREDICTED_PRICE_BATCH'] = predicted\nbostonDF.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}